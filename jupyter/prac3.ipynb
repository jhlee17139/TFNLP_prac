{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "!python -m pip install -U \"tensorflow-text==2.9.*\"\n",
    "!python -m pip install tensorflow-hub\n",
    "!python -m pip install tensorflow_datasets\n",
    "\n",
    "%cd /content/drive/MyDrive/2022_2_machine_learning_hw4/TFNLP_prac\n",
    "import sys\n",
    "sys.path.append('/content/drive/MyDrive/2022_2_machine_learning_hw4/TFNLP_prac')\n",
    "print(sys.path)\n",
    "\n",
    "import tensorflow as tf\n",
    "print(tf.config.list_physical_devices('GPU'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import argparse\n",
    "from dataset import build as dataset_build\n",
    "from model import build as model_build"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def parse_args():\n",
    "    parser = argparse.ArgumentParser(\n",
    "        description='Machine Learning Homework 4 : NLP')\n",
    "\n",
    "    # model setting\n",
    "    '''\n",
    "    word_embed : bert\n",
    "    encoder : bert\n",
    "    decoder : transformer\n",
    "    head : sentiment_head\n",
    "    '''\n",
    "    parser.add_argument('--word_embed', default='bert', type=str)\n",
    "    parser.add_argument('--encoder', default='bert', type=str)\n",
    "    parser.add_argument('--decoder', default='transformer', type=str)\n",
    "    parser.add_argument('--head', default='sentiment_head', type=str)\n",
    "\n",
    "    # dataset setting\n",
    "    # imdb or raw_imdb\n",
    "    parser.add_argument('--dataset', default='raw_imdb', type=str)\n",
    "\n",
    "    # imdb dataset setting\n",
    "    parser.add_argument('--n_words', default=10000, type=int)\n",
    "    parser.add_argument('--max_len', default=128, type=int)\n",
    "    # 256\n",
    "    parser.add_argument('--dim_embedding', default=256, type=int)\n",
    "\n",
    "    # encoder\n",
    "    parser.add_argument('--encoder_n_layer', default=2, type=int)\n",
    "\n",
    "    # cnn encoder\n",
    "    parser.add_argument('--cnn_kernel_size', default=3, type=int)\n",
    "\n",
    "    # transformer\n",
    "    parser.add_argument('--encoder_n_head', default=8, type=int)\n",
    "\n",
    "    # decoder\n",
    "    parser.add_argument('--decoder_n_layer', default=2, type=int)\n",
    "    parser.add_argument('--decoder_n_head', default=4, type=int)\n",
    "\n",
    "    # bert\n",
    "    parser.add_argument('--bert_model_name', default='small_bert/bert_en_uncased_L-4_H-256_A-4', type=str)\n",
    "\n",
    "    # hyper parameter\n",
    "    parser.add_argument('--drop_out', default=0.05, type=float)\n",
    "    parser.add_argument('--optimizer', default='adam', type=str)\n",
    "    parser.add_argument('--loss', default='binary_crossentropy', type=str)\n",
    "    parser.add_argument('--epochs', default=5, type=int)\n",
    "    parser.add_argument('--batch_size', default=500, type=int)\n",
    "    parser.add_argument('--seed', default=42, type=int)\n",
    "\n",
    "    return parser.parse_args(args=[])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    args = parse_args()\n",
    "    model = model_build.build_model(args)\n",
    "    model.summary()\n",
    "    model.compile(optimizer=args.optimizer, loss=args.loss, metrics=[\"accuracy\"])\n",
    "\n",
    "    if args.dataset == 'imdb':\n",
    "        (x_train, y_train), (x_test, y_test) = dataset_build.build_dataset(args)\n",
    "        score = model.fit(x_train, y_train,\n",
    "                          epochs=args.epochs,\n",
    "                          batch_size=args.batch_size,\n",
    "                          validation_data=(x_test, y_test))\n",
    "        print(\"\\nTest loss:\", score.history['val_loss'][-1])\n",
    "        print('Test accuracy:', score.history['val_accuracy'][-1])\n",
    "\n",
    "    elif args.dataset == 'raw_imdb':\n",
    "        train_ds, val_ds, test_ds = dataset_build.build_dataset(args)\n",
    "        score = model.fit(x=train_ds,\n",
    "                          validation_data=val_ds,\n",
    "                          epochs=args.epochs)\n",
    "        print(\"\\nTest loss:\", score.history['val_loss'][-1])\n",
    "        print('Test accuracy:', score.history['val_accuracy'][-1])\n",
    "\n",
    "    else:\n",
    "        print(\"args.dataset error\")\n",
    "        exit()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}